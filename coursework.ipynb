{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 852,
   "metadata": {
    "id": "Ya-q8foQbmze"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 853,
   "metadata": {
    "id": "35-Ah-bJahGz"
   },
   "outputs": [],
   "source": [
    "filename = \"log05\"\n",
    "# filename = \"log04\"\n",
    "\n",
    "sqlite_log_filename = 'data/' + filename + \".sq3\"\n",
    "tracks_filename = 'data/' + filename + \"_tracks.txt\"\n",
    "\n",
    "json_filename = 'output/' + filename + \".json\"\n",
    "dot_filename = 'output/' + filename + \".dot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 854,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_zu9ByB2ahLK",
    "outputId": "b6a7a26b-bc05-4cbc-b6a0-1753f326cef1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracks preprocessed\n"
     ]
    }
   ],
   "source": [
    "#Preprocess sqlite logs\n",
    "\n",
    "conn = sqlite3.connect(sqlite_log_filename)\n",
    "\n",
    "df = pd.read_sql_query('SELECT * FROM Events', conn)\n",
    "\n",
    "with open(tracks_filename, \"w\") as f:\n",
    "    for track_id, track in df.groupby(df.trace):\n",
    "        print(\"\".join(track.sort_values(by=[\"timest\"], ascending=True).activity.values), file=f)\n",
    "print(\"Tracks preprocessed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 855,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cY8RuTK2ajqV",
    "outputId": "893008e2-1284-49f6-83ee-59c2372beb87"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abcdef', 'abcdeg', 'abcdfe', 'abcdfg', 'abd', 'abdg', 'abdef', 'abdeg']"
      ]
     },
     "execution_count": 855,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_token = \"_\"\n",
    "\n",
    "with open(tracks_filename) as f:\n",
    "    tracks = f.read().splitlines()\n",
    "\n",
    "tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 856,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BQt9w7rpajs0",
    "outputId": "3a4a52bb-48f5-4370-8c5f-14af950226fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n samples =  8\n",
      "abcdef\n",
      "abcdeg\n",
      "abcdfe\n",
      "abcdfg\n",
      "abd\n",
      "abdg\n",
      "abdef\n",
      "abdeg\n"
     ]
    }
   ],
   "source": [
    "print ('n samples = ',len(tracks))\n",
    "for x in tracks:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 857,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t7KeWU5xajva",
    "outputId": "092cfe39-1e5d-4e74-c984-0d7819695cbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_tokens =  8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' ', 'a', 'b', 'c', 'd', 'e', 'f', 'g']"
      ]
     },
     "execution_count": 857,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def unique_tokens(tracks):\n",
    "    #all unique characters go here\n",
    "    token_set = set() #add start token\n",
    "    for track in tracks:\n",
    "        for symbol in track:\n",
    "            token_set.add(symbol)\n",
    "\n",
    "    return sorted(list(token_set) + [' '])\n",
    "  \n",
    "tokens = unique_tokens(tracks)\n",
    "\n",
    "\n",
    "print ('n_tokens = ', len(tokens))\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 858,
   "metadata": {
    "id": "sO-mry0tapgG"
   },
   "outputs": [],
   "source": [
    "#!token_to_id = <dictionary of symbol -> its identifier (index in tokens list)>\n",
    "def token2id(tokens):\n",
    "    return {t:i for i,t in enumerate(tokens) }\n",
    "\n",
    "token_to_id = token2id(tokens)\n",
    "\n",
    "#!id_to_token = < dictionary of symbol identifier -> symbol itself>\n",
    "def id2token(tokens):\n",
    "    return {i:t for i,t in enumerate(tokens)}\n",
    "\n",
    "id_to_token = id2token(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 859,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ffBH5VUOa7SL",
    "outputId": "2018aeca-3cc6-4a53-bad4-3efdbcb31b26"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3, 4, 5, 6, 0],\n",
       "       [1, 2, 3, 4, 5, 7, 0],\n",
       "       [1, 2, 3, 4, 6, 5, 0],\n",
       "       [1, 2, 3, 4, 6, 7, 0],\n",
       "       [1, 2, 4, 0, 0, 0, 0],\n",
       "       [1, 2, 4, 7, 0, 0, 0],\n",
       "       [1, 2, 4, 5, 6, 0, 0],\n",
       "       [1, 2, 4, 5, 7, 0, 0]])"
      ]
     },
     "execution_count": 859,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LEN = max(list(map(len, tracks)))\n",
    "tracks_ix = list(map(lambda track: list(map(token_to_id.get,track)),tracks))\n",
    "\n",
    "#pad short ones\n",
    "for i in range(len(tracks_ix)):\n",
    "    if len(tracks_ix[i]) < MAX_LEN:\n",
    "        tracks_ix[i] += [token_to_id[\" \"]]*(MAX_LEN - len(tracks_ix[i])) #pad too short\n",
    "    tracks_ix[i] += [token_to_id[\" \"]]\n",
    "        \n",
    "tracks_ix = np.array(tracks_ix)\n",
    "tracks_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 860,
   "metadata": {
    "id": "FGNQw2hja7VF"
   },
   "outputs": [],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 861,
   "metadata": {
    "id": "OP2jTB6QbBWL"
   },
   "outputs": [],
   "source": [
    "# Transformed version\n",
    "class CustomRNN_2(nn.Module):\n",
    "    def __init__(self, input_size, emb_size, hidden_size, n_neurons):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        #Embedding\n",
    "        self.embedding = nn.Embedding(input_size, emb_size) # получаем на вход одно событие на каждом шаге?\n",
    "        #First layer with activation function ReLU\n",
    "        self.linear1 = nn.Linear(hidden_size+emb_size, 8)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        # Second layer, linear\n",
    "        self.linear2 = nn.Linear(8, n_neurons)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        # self.gumbel = F.gumbel_softmax(tau=t)\n",
    "\n",
    "        # Output layer, returns probabilities for the next token\n",
    "        self.linear3 = nn.Linear(in_features=n_neurons, out_features=input_size)\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, x, hidden_state, t=0.1):\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        x = torch.cat([x, hidden_state], 1)\n",
    "\n",
    "        x = self.relu1(self.linear1(x))\n",
    "\n",
    "        x = self.linear2(x)\n",
    "\n",
    "        hidden_state = F.gumbel_softmax(logits=torch.tensor(x), tau=t)\n",
    "        # hidden_state = self.softmax(x)\n",
    "        out = self.sigmoid(x)\n",
    "\n",
    "        out = self.softmax(self.linear3(out))\n",
    "\n",
    "        return out, hidden_state\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return nn.init.zeros_(torch.empty(batch_size, self.hidden_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 862,
   "metadata": {
    "id": "2_HghNCFbDOO"
   },
   "outputs": [],
   "source": [
    "input_size = len(tokens)\n",
    "emb_size = 30\n",
    "hidden_size = n_neurons = 2\n",
    "t = 0.1\n",
    "learning_rate = 0.001\n",
    "\n",
    "model_try2 = CustomRNN_2(input_size=input_size, emb_size=emb_size, hidden_size=hidden_size, n_neurons=n_neurons)\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model_try2.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 863,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6asf6EdQbQp_",
    "outputId": "0575614e-7bbe-4a1d-97c8-5a36b7cc2def"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding.weight torch.Size([8, 30])\n",
      "linear1.weight torch.Size([8, 32])\n",
      "linear1.bias torch.Size([8])\n",
      "linear2.weight torch.Size([2, 8])\n",
      "linear2.bias torch.Size([2])\n",
      "linear3.weight torch.Size([8, 2])\n",
      "linear3.bias torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model_try2.state_dict().items():\n",
    "    print(name, param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 864,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\efe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "c:\\users\\efe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch 0 average loss = 90.25003814697266\n",
      "\n",
      "\n",
      "Epoch 1 average loss = 79.5781021118164\n",
      "\n",
      "\n",
      "Epoch 2 average loss = 73.15046691894531\n",
      "\n",
      "\n",
      "Epoch 3 average loss = 66.95634460449219\n",
      "\n",
      "\n",
      "Epoch 4 average loss = 62.955543518066406\n",
      "\n",
      "\n",
      "Epoch 5 average loss = 58.364654541015625\n",
      "\n",
      "\n",
      "Epoch 6 average loss = 56.30702209472656\n",
      "\n",
      "\n",
      "Epoch 7 average loss = 54.152156829833984\n",
      "\n",
      "\n",
      "Epoch 8 average loss = 52.020137786865234\n",
      "\n",
      "\n",
      "Epoch 9 average loss = 51.537513732910156\n",
      "\n",
      "\n",
      "Epoch 10 average loss = 50.26982879638672\n",
      "\n",
      "\n",
      "Epoch 11 average loss = 48.533016204833984\n",
      "\n",
      "\n",
      "Epoch 12 average loss = 47.342559814453125\n",
      "\n",
      "\n",
      "Epoch 13 average loss = 46.301734924316406\n",
      "\n",
      "\n",
      "Epoch 14 average loss = 45.99565505981445\n",
      "\n",
      "\n",
      "Epoch 15 average loss = 45.06922912597656\n",
      "\n",
      "\n",
      "Epoch 16 average loss = 44.70333480834961\n",
      "\n",
      "\n",
      "Epoch 17 average loss = 43.43256759643555\n",
      "\n",
      "\n",
      "Epoch 18 average loss = 42.8145866394043\n",
      "\n",
      "\n",
      "Epoch 19 average loss = 42.198795318603516\n",
      "\n",
      "\n",
      "Epoch 20 average loss = 39.860443115234375\n",
      "\n",
      "\n",
      "Epoch 21 average loss = 38.8641471862793\n",
      "\n",
      "\n",
      "Epoch 22 average loss = 37.48319625854492\n",
      "\n",
      "\n",
      "Epoch 23 average loss = 36.460689544677734\n",
      "\n",
      "\n",
      "Epoch 24 average loss = 35.16255569458008\n"
     ]
    }
   ],
   "source": [
    "def sample_batch(data, batch_size):\n",
    "    rows = data[np.random.randint(0,len(data),size=batch_size)]\n",
    "    return rows\n",
    "\n",
    "#total N iterations\n",
    "n_epochs = 25\n",
    "\n",
    "# how many minibatches are there in the epoch \n",
    "batches_per_epoch = 100\n",
    "\n",
    "#how many training sequences are processed in a single function call\n",
    "batch_size= 10\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    avg_cost = 0\n",
    "    for _ in range(batches_per_epoch):\n",
    "        loss = 0;\n",
    "        train_try = sample_batch(torch.tensor(tracks_ix), batch_size)\n",
    "        train_try = train_try.type(torch.LongTensor)\n",
    "        mask = torch.ne(train_try[:, 1:], token_to_id[\" \"]) # we want to skip padded tokens in our track, when computing loss\n",
    "        hidden_state = model_try2.init_hidden(batch_size)\n",
    "#         pred_track = torch.zeros(size=(batch_size, len(tokens), len(train_try[0])-1), dtype=torch.float32)\n",
    "        for j in range(len(train_try[0])-1):\n",
    "            output, hidden_state = model_try2(train_try[:, j], hidden_state)\n",
    "            for i in range(batch_size):\n",
    "                loss += criterion(torch.log(output[i]), train_try[i, j+1]) * mask[i, j]\n",
    "#             pred_track[:, :, j] = output\n",
    "#             print(output)\n",
    "#             print(pred_track)\n",
    "        avg_cost += loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        nn.utils.clip_grad_norm_(model_try2.parameters(), 1)\n",
    "        optimizer.step()\n",
    "    print(\"\\n\\nEpoch {} average loss = {}\".format(epoch, avg_cost / batches_per_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 865,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version for output\n",
    "class CustomRNN_2_output(nn.Module):\n",
    "    def __init__(self, input_size, emb_size, hidden_size, n_neurons):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        #Embedding\n",
    "        self.embedding = nn.Embedding(input_size, emb_size)\n",
    "        # self.embedding = nn.Embedding(1, emb_size) \n",
    "\n",
    "        #First layer with activation function ReLU\n",
    "        self.linear1 = nn.Linear(hidden_size+emb_size, 8)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        # Second layer, linear\n",
    "        self.linear2 = nn.Linear(8, n_neurons)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        # self.gumbel = F.gumbel_softmax(tau=t)\n",
    "\n",
    "        # Output layer, returns probabilities for the next token\n",
    "        self.linear3 = nn.Linear(in_features=n_neurons, out_features=input_size)\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, x, hidden_state, t=0.1):\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        x = torch.cat([x, hidden_state], 1)\n",
    "\n",
    "        x = self.relu1(self.linear1(x))\n",
    "\n",
    "        x = self.linear2(x)\n",
    "\n",
    "        # hidden_state = F.gumbel_softmax(logits=torch.tensor(x), tau=t)\n",
    "        hidden_state = torch.gt(x, 0.5)\n",
    "        # hidden_state = self.softmax(x)\n",
    "        out = self.sigmoid(x)\n",
    "\n",
    "        out = self.softmax(self.linear3(out))\n",
    "\n",
    "        return out, hidden_state\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return nn.init.zeros_(torch.empty(batch_size, self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 866,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = len(tokens)\n",
    "emb_size = 30\n",
    "hidden_size = n_neurons = 2\n",
    "t = 0.1\n",
    "learning_rate = 0.001\n",
    "\n",
    "model_output = CustomRNN_2_output(input_size=input_size, emb_size=emb_size, hidden_size=hidden_size, n_neurons=n_neurons)\n",
    "criterion = nn.CrossEntropyLoss() # Make a choice\n",
    "optimizer = torch.optim.Adam(model_try2.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 867,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output.embedding.weight = model_try2.embedding.weight\n",
    "\n",
    "model_output.linear1.weight = model_try2.linear1.weight\n",
    "model_output.linear1.bias = model_try2.linear1.bias\n",
    "\n",
    "model_output.linear2.weight = model_try2.linear2.weight\n",
    "model_output.linear2.bias = model_try2.linear2.bias\n",
    "\n",
    "model_output.linear3.weight = model_try2.linear3.weight\n",
    "model_output.linear3.bias = model_try2.linear3.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 868,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\efe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:39: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "\n",
    "n_traces = len(tracks_ix)\n",
    "hidden_state = model_output.init_hidden(n_traces)\n",
    "train_try = torch.tensor(tracks_ix)\n",
    "# final_res = np.empty(shape=(8, 7))\n",
    "# hidden_final = np.zeros(shape=(8, 6, n_neurons))\n",
    "final_res = np.empty(shape=(n_traces, len(train_try[0])))\n",
    "hidden_final = np.zeros(shape=(n_traces, len(train_try[0])-1, n_neurons))\n",
    "for c in range(n_traces):\n",
    "    final_res[c][0] = 1\n",
    "    # print(final_res)\n",
    "for j in range(0, len(train_try[0])-1):\n",
    "    output, hidden_state = model_output(train_try[:, j], hidden_state)\n",
    "    # print(output)\n",
    "#     print(hidden_state[0])\n",
    "    for i in range(0, len(output)):\n",
    "        pred = list(output[i]).index(max(output[i]))\n",
    "        final_res[i][j+1] = int(pred)\n",
    "        hidden_final[i][j] = hidden_state[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 869,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 870,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# final_res[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5Eh9aXnabras",
    "outputId": "4e1bca41-f31e-4dc0-f91d-c1f555aed05c"
   },
   "outputs": [],
   "source": [
    "# train_try[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 872,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tracks_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 873,
   "metadata": {
    "id": "2RXkmdztcxvm"
   },
   "outputs": [],
   "source": [
    "def binary_state_to_id(binary_state):\n",
    "    return str(int(sum(round(val)*2**(index) for index, val in enumerate(binary_state))))\n",
    "\n",
    "def num2state(states):\n",
    "    return {j:\"s\"+str(i) for i,j in enumerate(states)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 874,
   "metadata": {
    "id": "AQIwgffvcLut"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def build_json(filename, states): \n",
    "    graph = {\"states\":set(), \"transitions\":list(), \"meta\":{\"isAccepting\":set()}}\n",
    "    all_states = []\n",
    "    all_used_states = set()\n",
    "    \n",
    "    # inferred_states = infer_states(tracks_ix)\n",
    "    inferred_states = states\n",
    "\n",
    "    for binary_states in inferred_states:\n",
    "        binary_states = np.vstack((np.zeros(n_neurons), binary_states)) # add initial state\n",
    "        states = list(map(binary_state_to_id, binary_states))\n",
    "        graph[\"states\"].update(set(states))\n",
    "        all_states.append(states)\n",
    "    num_to_state = num2state(np.unique(all_states))\n",
    "    all_states = list(map(lambda states: list(map(num_to_state.get,states)),all_states))\n",
    "    \n",
    "    for states, track_ids in zip(all_states, tracks_ix):\n",
    "        track = \"\".join(list(map(id_to_token.get, track_ids))).strip()\n",
    "        for index, symbol in enumerate(track):\n",
    "            transition = {\"from\":states[index], \"to\":states[index + 1], \"track\":symbol}\n",
    "            all_used_states.add(transition[\"from\"])\n",
    "            all_used_states.add(transition[\"to\"])\n",
    "            if transition not in graph[\"transitions\"]:\n",
    "                graph[\"transitions\"].append(transition)  \n",
    "        graph[\"meta\"][\"isAccepting\"].add(states[len(track)])\n",
    "        \n",
    "    graph[\"states\"] = list(all_used_states)\n",
    "    graph[\"meta\"][\"isAccepting\"] = list(graph[\"meta\"][\"isAccepting\"])\n",
    "    graph[\"meta\"][\"tracksNum\"] = len(tracks)\n",
    "    with open(filename, \"w\") as json_file:\n",
    "        json.dump(graph, json_file)\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 875,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TEST\n",
    "# graph = {\"states\":set(), \"transitions\":list(), \"meta\":{\"isAccepting\":set()}}\n",
    "# all_states = []\n",
    "# all_used_states = set()\n",
    "\n",
    "# inferred_states = hidden_final\n",
    "\n",
    "# for binary_states in inferred_states:\n",
    "#     binary_states = np.vstack((np.zeros(n_neurons), binary_states)) # add initial state\n",
    "#     states = list(map(binary_state_to_id, binary_states))\n",
    "#     graph[\"states\"].update(set(states))\n",
    "#     all_states.append(states)\n",
    "    \n",
    "# print(all_states)\n",
    "\n",
    "# def num2state(states):\n",
    "#     return {j:\"s\"+str(i) for i,j in enumerate(states)}\n",
    "\n",
    "# num_to_state = num2state(np.unique(all_states))\n",
    "# print(num_to_state)\n",
    "# all_states_enc = list(map(lambda states: list(map(num_to_state.get,states)),all_states))\n",
    "# print(all_states_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 876,
   "metadata": {
    "id": "rpEokj9fcyUX"
   },
   "outputs": [],
   "source": [
    "graph = build_json(json_filename, hidden_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 877,
   "metadata": {
    "id": "E8P89lSZOkx_"
   },
   "outputs": [],
   "source": [
    "def build_dot(json_graph, filename):\n",
    "    graph = \"digraph test {\\n\"\n",
    "    for state in json_graph[\"states\"]:\n",
    "        if state in json_graph[\"meta\"][\"isAccepting\"]:\n",
    "            graph += \"\\t\" + state + \" [shape=doublecircle];\\n\"\n",
    "        else:\n",
    "            graph += \"\\t\" + state + \";\\n\"\n",
    "    for transition in json_graph[\"transitions\"]:\n",
    "        graph += \"\\t\" + transition[\"from\"] + \" -> \" + transition[\"to\"] \n",
    "        graph += \" [label=\\\"\" + transition[\"track\"] + \"\\\"];\\n\"\n",
    "    graph += \"}\" \n",
    "    \n",
    "    with open(filename, \"w\") as graph_file:\n",
    "        print(graph, file=graph_file, end=\"\")\n",
    "        \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 878,
   "metadata": {
    "id": "mVkMedMyUE75"
   },
   "outputs": [],
   "source": [
    "dot_graph = build_dot(graph, dot_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 879,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cu6FnmBYUGHg",
    "outputId": "823224ed-06b5-47fc-8070-5cb65240dbf9"
   },
   "outputs": [],
   "source": [
    "!dot output/log05.dot -Tpng -o output/log05_2neurons.png\n",
    "\n",
    "# !dot output/log04.dot -Tpng -o output/log04.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ymq4NZDgUIvs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "coursework.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
